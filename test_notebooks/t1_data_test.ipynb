{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "680b0ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å•å…ƒæ ¼ 1: å¯¼å…¥\n",
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "import scanpy as sc\n",
    "import anndata\n",
    "import webdataset as wds\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import io\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# å…³é”®ï¼šå¯¼å…¥PyTorch Geometric\n",
    "import torch.utils.data\n",
    "from torch_geometric.data import Data as PyGData\n",
    "from torch_geometric.loader import DataLoader as PyGDataLoader # æˆ–è€…ç›´æ¥ç”¨torchçš„Dataloader + PyGçš„collate_fn\n",
    "from torch_geometric.data import Batch as PyGBatch\n",
    "\n",
    "# ä»æ‚¨å‡†å¤‡è¦ä¿®æ”¹çš„open-clipç›®å½•ä¸­å¯¼å…¥ä¸€äº›è¾…åŠ©ç±»\n",
    "# å¦‚æœæ‚¨çš„notebookå’Œopen-clipåº“ä¸åœ¨åŒä¸€ä¸ªç¯å¢ƒä¸‹ï¼Œæ‚¨å¯èƒ½éœ€è¦è°ƒæ•´sys.path\n",
    "# from open_clip_train.data import DataInfo, SharedEpoch \n",
    "\n",
    "# ä¸ºäº†è®©Notebookè‡ªåŒ…å«ï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œé‡æ–°å®šä¹‰è¿™äº›ç®€å•çš„è¾…åŠ©ç±»\n",
    "from dataclasses import dataclass\n",
    "@dataclass\n",
    "class DataInfo:\n",
    "    dataloader: torch.utils.data.DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef9a290f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æµ‹è¯•ç¯å¢ƒç›®å½• '/cwStorage/nodecw_group/jijh/openclip_train/spaglam_test_environment' å·²åˆ›å»ºã€‚\n",
      "\n",
      "ğŸ› ï¸ æ­£åœ¨ç”Ÿæˆæ¨¡æ‹Ÿ AnnData æ–‡ä»¶...\n",
      "âœ… æ¨¡æ‹Ÿ AnnData æ–‡ä»¶å·²ä¿å­˜è‡³: /cwStorage/nodecw_group/jijh/openclip_train/spaglam_test_environment/metadata/dummy_master_adata.h5ad\n",
      "\n",
      "ğŸ› ï¸ æ­£åœ¨ç”Ÿæˆæ¨¡æ‹Ÿ WebDataset åˆ†ç‰‡...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ead48c6df8947bd8c63498c275c19b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "æ‰“åŒ…æ¨¡æ‹Ÿæ•°æ®:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æ¨¡æ‹Ÿ WebDataset åˆ†ç‰‡å·²ä¿å­˜è‡³: /cwStorage/nodecw_group/jijh/openclip_train/spaglam_test_environment/shards/shard-000000.tar\n"
     ]
    }
   ],
   "source": [
    "# å•å…ƒæ ¼ 2: åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®å’Œæ–‡ä»¶ç»“æ„\n",
    "\n",
    "# --- é…ç½® ---\n",
    "TEST_ENV_DIR = \"/cwStorage/nodecw_group/jijh/openclip_train/spaglam_test_environment\"\n",
    "SHARDS_DIR = os.path.join(TEST_ENV_DIR, \"shards\")\n",
    "METADATA_DIR = os.path.join(TEST_ENV_DIR, \"metadata\")\n",
    "DUMMY_ADATA_PATH = os.path.join(METADATA_DIR, \"dummy_master_adata.h5ad\")\n",
    "DUMMY_SHARD_PATH_PATTERN = os.path.join(SHARDS_DIR, \"shard-%06d.tar\")\n",
    "\n",
    "NUM_SPOTS = 100\n",
    "NUM_NEIGHBORS = 6\n",
    "IMAGE_SIZE = (224, 224)\n",
    "\n",
    "# --- æ¸…ç†å¹¶åˆ›å»ºç›®å½• ---\n",
    "if os.path.exists(TEST_ENV_DIR):\n",
    "    shutil.rmtree(TEST_ENV_DIR)\n",
    "os.makedirs(SHARDS_DIR, exist_ok=True)\n",
    "os.makedirs(METADATA_DIR, exist_ok=True)\n",
    "print(f\"âœ… æµ‹è¯•ç¯å¢ƒç›®å½• '{TEST_ENV_DIR}' å·²åˆ›å»ºã€‚\")\n",
    "\n",
    "# --- 1. ç”Ÿæˆæ¨¡æ‹ŸAnnDataæ–‡ä»¶ ---\n",
    "print(\"\\nğŸ› ï¸ æ­£åœ¨ç”Ÿæˆæ¨¡æ‹Ÿ AnnData æ–‡ä»¶...\")\n",
    "spot_ids = [f\"DUMMY_SPOT_{i}\" for i in range(NUM_SPOTS)]\n",
    "adata = anndata.AnnData(X=np.random.rand(NUM_SPOTS, 10)) # ä¼ªåŸºå› è¡¨è¾¾æ•°æ®\n",
    "adata.obs_names = spot_ids\n",
    "adata.obs['sample_id'] = 'DUMMY_SAMPLE'\n",
    "# ä¸ºæ¯ä¸ªspotç”ŸæˆæŒ‡å‘å³å°†åˆ›å»ºçš„æ–‡ä»¶çš„è·¯å¾„\n",
    "adata.obs['image_path'] = [os.path.join(SHARDS_DIR, f\"{sid}.png\") for sid in spot_ids]\n",
    "adata.obs['sentence_path'] = [os.path.join(SHARDS_DIR, f\"{sid}.txt\") for sid in spot_ids]\n",
    "adata.obsm['spatial'] = np.random.rand(NUM_SPOTS, 2) * 1000 # ä¼ªç©ºé—´åæ ‡\n",
    "\n",
    "# åˆ›å»ºä¸€ä¸ªç®€å•çš„k-NNå›¾\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "connectivity_matrix = kneighbors_graph(adata.obsm['spatial'], NUM_NEIGHBORS, mode='connectivity', include_self=False)\n",
    "adata.obsp['spatial_connectivities'] = connectivity_matrix\n",
    "adata.write_h5ad(DUMMY_ADATA_PATH)\n",
    "print(f\"âœ… æ¨¡æ‹Ÿ AnnData æ–‡ä»¶å·²ä¿å­˜è‡³: {DUMMY_ADATA_PATH}\")\n",
    "\n",
    "# --- 2. ç”Ÿæˆæ¨¡æ‹ŸWebDatasetåˆ†ç‰‡ ---\n",
    "print(\"\\nğŸ› ï¸ æ­£åœ¨ç”Ÿæˆæ¨¡æ‹Ÿ WebDataset åˆ†ç‰‡...\")\n",
    "shard_path = DUMMY_SHARD_PATH_PATTERN % 0\n",
    "with wds.TarWriter(shard_path) as sink:\n",
    "    for spot_id in tqdm(spot_ids, desc=\"æ‰“åŒ…æ¨¡æ‹Ÿæ•°æ®\"):\n",
    "        # åˆ›å»ºä¸€ä¸ªå¸¦æ–‡å­—çš„ä¼ªå›¾åƒ\n",
    "        img = Image.new('RGB', IMAGE_SIZE, color = 'black')\n",
    "        draw = ImageDraw.Draw(img)\n",
    "        try:\n",
    "            # å°è¯•åŠ è½½ä¸€ä¸ªå­—ä½“ï¼Œå¦‚æœå¤±è´¥åˆ™ä½¿ç”¨é»˜è®¤\n",
    "            font = ImageFont.truetype(\"arial.ttf\", 15)\n",
    "        except IOError:\n",
    "            font = ImageFont.load_default()\n",
    "        draw.text((10, 10), spot_id, fill='white', font=font)\n",
    "        \n",
    "        # å°†å›¾åƒä¿å­˜åˆ°å†…å­˜ä¸­çš„å­—èŠ‚æµ\n",
    "        img_byte_arr = io.BytesIO()\n",
    "        img.save(img_byte_arr, format='PNG')\n",
    "        image_data = img_byte_arr.getvalue()\n",
    "\n",
    "        # åˆ›å»ºä¼ªåŸºå› è¯­å¥\n",
    "        sentence_data = f\"GENES FOR {spot_id}: GENE_A GENE_B GENE_C\".encode('utf-8')\n",
    "        \n",
    "        # å†™å…¥æ ·æœ¬\n",
    "        sample = {\"__key__\": spot_id, \"png\": image_data, \"txt\": sentence_data}\n",
    "        sink.write(sample)\n",
    "        \n",
    "        # åŒæ—¶ä¿å­˜ä¼ªPNGå’ŒTXTæ–‡ä»¶ï¼Œä¾›AnnDataä¸­çš„è·¯å¾„å¼•ç”¨\n",
    "        with open(os.path.join(SHARDS_DIR, f\"{spot_id}.png\"), \"wb\") as f_img: f_img.write(image_data)\n",
    "        with open(os.path.join(SHARDS_DIR, f\"{spot_id}.txt\"), \"wb\") as f_txt: f_txt.write(sentence_data)\n",
    "\n",
    "print(f\"âœ… æ¨¡æ‹Ÿ WebDataset åˆ†ç‰‡å·²ä¿å­˜è‡³: {shard_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35d056a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æ ¸å¿ƒæ•°æ®å¤„ç†ä»£ç å·²åŠ è½½åˆ°Notebookä¸­ã€‚\n"
     ]
    }
   ],
   "source": [
    "# å•å…ƒæ ¼ 3: å¤åˆ¶å¹¶ç²˜è´´æˆ‘ä»¬æ­£åœ¨æµ‹è¯•çš„æ ¸å¿ƒä»£ç \n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# è¿™æ˜¯æ‚¨è®¡åˆ’æ”¾åœ¨ src/open_clip_train/spaglam_data.py çš„ä»£ç \n",
    "# -------------------------------------------------------------\n",
    "import logging\n",
    "\n",
    "class SpaGLaMWDataset(torch.utils.data.IterableDataset):\n",
    "    def __init__(self, tar_urls, anndata_path, image_processor, tokenizer):\n",
    "        super().__init__()\n",
    "        self.tar_urls = tar_urls\n",
    "        self.anndata_path = anndata_path\n",
    "        self.image_processor = image_processor\n",
    "        self.tokenizer = tokenizer\n",
    "        self.adata = None\n",
    "\n",
    "    def _init_worker(self):\n",
    "        if self.adata is None:\n",
    "            self.adata = sc.read_h5ad(self.anndata_path, backed='r')\n",
    "\n",
    "    def _process_sample(self, sample):\n",
    "        self._init_worker()\n",
    "        center_key = sample['__key__']\n",
    "        \n",
    "        try:\n",
    "            center_idx = self.adata.obs_names.get_loc(center_key)\n",
    "            neighbor_indices = self.adata.obsp['spatial_connectivities'][center_idx].indices\n",
    "            neighbor_keys = self.adata.obs_names[neighbor_indices].tolist()\n",
    "            \n",
    "            all_keys = [center_key] + neighbor_keys\n",
    "            num_nodes = len(all_keys)\n",
    "\n",
    "            paths_df = self.adata.obs.loc[all_keys, ['image_path', 'sentence_path']]\n",
    "            \n",
    "            images, sentences = [], []\n",
    "            for key in all_keys:\n",
    "                if key == center_key:\n",
    "                    img = Image.open(io.BytesIO(sample['png'])).convert(\"RGB\")\n",
    "                    sent = sample['txt'].decode('utf-8')\n",
    "                else:\n",
    "                    img_path, sent_path = paths_df.loc[key]\n",
    "                    try: img = Image.open(img_path).convert(\"RGB\")\n",
    "                    except (FileNotFoundError, OSError): img = Image.new('RGB', (224, 224), color='grey')\n",
    "                    try: \n",
    "                        with open(sent_path, 'r') as f: sent = f.read()\n",
    "                    except (FileNotFoundError, OSError): sent = \"\"\n",
    "                images.append(self.image_processor(img))\n",
    "                sentences.append(sent)\n",
    "\n",
    "            tokenized_texts = self.tokenizer(sentences)\n",
    "            \n",
    "            key_to_local_idx = {key: i for i, key in enumerate(all_keys)}\n",
    "            edge_list = []\n",
    "            center_local_idx = 0\n",
    "            \n",
    "            # ä½¿ç”¨AnnDataä¸­çš„é‚»æ¥çŸ©é˜µæ„å»ºå±€éƒ¨å›¾\n",
    "            for i, key in enumerate(all_keys):\n",
    "                global_idx = self.adata.obs_names.get_loc(key)\n",
    "                # è·å–è¯¥èŠ‚ç‚¹çš„é‚»å±…çš„å…¨å±€ç´¢å¼•\n",
    "                row_indices = self.adata.obsp['spatial_connectivities'][global_idx].indices\n",
    "                \n",
    "                for neighbor_global_idx in row_indices:\n",
    "                    neighbor_key = self.adata.obs_names[neighbor_global_idx]\n",
    "                    if neighbor_key in key_to_local_idx:\n",
    "                        # ç¡®ä¿åªæ·»åŠ å­å›¾å†…éƒ¨çš„è¾¹\n",
    "                        j = key_to_local_idx[neighbor_key]\n",
    "                        edge_list.append([i, j])\n",
    "\n",
    "            edge_index = torch.tensor(edge_list, dtype=torch.long).t().contiguous() if edge_list else torch.empty((2, 0), dtype=torch.long)\n",
    "\n",
    "            return PyGData(\n",
    "                x_image=torch.stack(images),\n",
    "                x_text=tokenized_texts,\n",
    "                edge_index=edge_index,\n",
    "                num_nodes=num_nodes,\n",
    "                center_key=center_key,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Skipping sample {center_key} due to error: {e}\")\n",
    "            return None\n",
    "\n",
    "    def __iter__(self):\n",
    "        pipeline = wds.DataPipeline(\n",
    "            wds.SimpleShardList(self.tar_urls) if not isinstance(self.tar_urls, wds.ShardList) else self.tar_urls,\n",
    "            wds.split_by_worker,\n",
    "            wds.tarfile_to_samples(),\n",
    "            wds.decode(\"pilrgb\", \"txt\"),\n",
    "            wds.map(self._process_sample),\n",
    "            wds.select(lambda x: x is not None),\n",
    "        )\n",
    "        return iter(pipeline)\n",
    "\n",
    "def get_spaglam_dataset(args, preprocess_fn, is_train, epoch=0, tokenizer=None):\n",
    "    input_shards = args.train_data\n",
    "    dataset = SpaGLaMWDataset(\n",
    "        tar_urls=input_shards,\n",
    "        anndata_path=args.anndata_path,\n",
    "        image_processor=preprocess_fn,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        num_workers=args.workers,\n",
    "        collate_fn=PyGBatch.from_data_list,\n",
    "        drop_last=is_train,\n",
    "    )\n",
    "    # æ¨¡æ‹Ÿé™„åŠ å…ƒæ•°æ®\n",
    "    dataloader.num_samples = args.train_num_samples\n",
    "    if dataloader.num_samples:\n",
    "        dataloader.num_batches = dataloader.num_samples // args.batch_size\n",
    "    else:\n",
    "        dataloader.num_batches = 0\n",
    "    return DataInfo(dataloader=dataloader)\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# è¿™æ˜¯æ‚¨è®¡åˆ’ä¿®æ”¹çš„ src/open_clip_train/data.py çš„æ ¸å¿ƒéƒ¨åˆ†\n",
    "# -------------------------------------------------------------\n",
    "def get_dataset_fn(data_path, dataset_type):\n",
    "    if dataset_type == \"spaglam\":\n",
    "        return get_spaglam_dataset\n",
    "    # åœ¨è¿™ä¸ªæµ‹è¯•ä¸­æˆ‘ä»¬ä¸å…³å¿ƒå…¶ä»–ç±»å‹\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported dataset type for this test: {dataset_type}\")\n",
    "\n",
    "def get_data(args, preprocess_fns, epoch=0, tokenizer=None):\n",
    "    preprocess_train, preprocess_val = preprocess_fns\n",
    "    data = {}\n",
    "    if args.train_data:\n",
    "        data[\"train\"] = get_dataset_fn(args.train_data, args.dataset_type)(\n",
    "            args, preprocess_train, is_train=True, epoch=epoch, tokenizer=tokenizer)\n",
    "    return data\n",
    "\n",
    "print(\"âœ… æ ¸å¿ƒæ•°æ®å¤„ç†ä»£ç å·²åŠ è½½åˆ°Notebookä¸­ã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef816aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- æ¨¡æ‹Ÿå‚æ•° ---\n",
      "dataset_type: spaglam\n",
      "train_data: ['/cwStorage/nodecw_group/jijh/openclip_train/spaglam_test_environment/shards/shard-000000.tar']\n",
      "anndata_path: /cwStorage/nodecw_group/jijh/openclip_train/spaglam_test_environment/metadata/dummy_master_adata.h5ad\n",
      "train_num_samples: 100\n",
      "batch_size: 4\n",
      "workers: 0\n",
      "world_size: 1\n",
      "distributed: False\n",
      "\n",
      "âœ… æ¨¡æ‹Ÿçš„é¢„å¤„ç†å™¨å’Œåˆ†è¯å™¨å·²å‡†å¤‡å°±ç»ªã€‚\n",
      "\n",
      "--- è°ƒç”¨ get_data() ---\n",
      "âœ… æˆåŠŸåˆ›å»ºDataLoaderï¼\n",
      "\n",
      "--- æœ€ç»ˆéªŒè¯ï¼šä»DataLoaderä¸­è·å–ä¸€ä¸ªæ‰¹æ¬¡ ---\n",
      "\n",
      "âŒ [å¤±è´¥] åœ¨è·å–æˆ–éªŒè¯æ‰¹æ¬¡æ—¶å‘ç”Ÿé”™è¯¯: module 'webdataset' has no attribute 'ShardList'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_152893/2261647055.py\", line 41, in <module>\n",
      "    batch = next(iter(dataloader))\n",
      "                 ~~~~^^^^^^^^^^^^\n",
      "  File \"/public/home/jijh/micromamba/envs/gigapath/lib/python3.13/site-packages/torch/utils/data/dataloader.py\", line 491, in __iter__\n",
      "    return self._get_iterator()\n",
      "           ~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/public/home/jijh/micromamba/envs/gigapath/lib/python3.13/site-packages/torch/utils/data/dataloader.py\", line 419, in _get_iterator\n",
      "    return _SingleProcessDataLoaderIter(self)\n",
      "  File \"/public/home/jijh/micromamba/envs/gigapath/lib/python3.13/site-packages/torch/utils/data/dataloader.py\", line 754, in __init__\n",
      "    self._dataset_fetcher = _DatasetKind.create_fetcher(\n",
      "                            ~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        self._dataset_kind,\n",
      "        ^^^^^^^^^^^^^^^^^^^\n",
      "    ...<3 lines>...\n",
      "        self._drop_last,\n",
      "        ^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/public/home/jijh/micromamba/envs/gigapath/lib/python3.13/site-packages/torch/utils/data/dataloader.py\", line 81, in create_fetcher\n",
      "    return _utils.fetch._IterableDatasetFetcher(\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        dataset, auto_collation, collate_fn, drop_last\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/public/home/jijh/micromamba/envs/gigapath/lib/python3.13/site-packages/torch/utils/data/_utils/fetch.py\", line 22, in __init__\n",
      "    self.dataset_iter = iter(dataset)\n",
      "                        ~~~~^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_152893/3162152864.py\", line 84, in __iter__\n",
      "    wds.SimpleShardList(self.tar_urls) if not isinstance(self.tar_urls, wds.ShardList) else self.tar_urls,\n",
      "                                                                        ^^^^^^^^^^^^^\n",
      "AttributeError: module 'webdataset' has no attribute 'ShardList'. Did you mean: 'shardlists'?\n"
     ]
    }
   ],
   "source": [
    "# å•å…ƒæ ¼ 4: æ¨¡æ‹Ÿè®­ç»ƒè„šæœ¬çš„è°ƒç”¨å¹¶éªŒè¯è¾“å‡º\n",
    "\n",
    "# --- 1. æ¨¡æ‹Ÿå‘½ä»¤è¡Œå‚æ•° (args) ---\n",
    "from argparse import Namespace\n",
    "args = Namespace(\n",
    "    dataset_type='spaglam',\n",
    "    train_data=[DUMMY_SHARD_PATH_PATTERN % 0], # ä½¿ç”¨æˆ‘ä»¬åˆ›å»ºçš„æ¨¡æ‹Ÿåˆ†ç‰‡\n",
    "    anndata_path=DUMMY_ADATA_PATH,            # ä½¿ç”¨æˆ‘ä»¬åˆ›å»ºçš„æ¨¡æ‹ŸAnnData\n",
    "    train_num_samples=NUM_SPOTS,\n",
    "    batch_size=4,\n",
    "    workers=0, # å…³é”®ï¼šåœ¨notebookä¸­è®¾ä¸º0ï¼Œä»¥ä¾¿äºè°ƒè¯•ï¼Œé¿å…å¤šè¿›ç¨‹é—®é¢˜\n",
    "    world_size=1,\n",
    "    distributed=False\n",
    ")\n",
    "print(\"--- æ¨¡æ‹Ÿå‚æ•° ---\")\n",
    "for k, v in vars(args).items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "# --- 2. æ¨¡æ‹Ÿé¢„å¤„ç†å™¨å’Œåˆ†è¯å™¨ ---\n",
    "from torchvision import transforms\n",
    "dummy_preprocess_fn = transforms.Compose([\n",
    "    transforms.Resize(IMAGE_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# ä¸€ä¸ªéå¸¸ç®€å•çš„åˆ†è¯å™¨ï¼Œè¿”å›å›ºå®šé•¿åº¦çš„éšæœºæ•´æ•°å¼ é‡\n",
    "dummy_tokenizer = lambda texts: torch.randint(0, 49408, (len(texts), 77), dtype=torch.long)\n",
    "print(\"\\nâœ… æ¨¡æ‹Ÿçš„é¢„å¤„ç†å™¨å’Œåˆ†è¯å™¨å·²å‡†å¤‡å°±ç»ªã€‚\")\n",
    "\n",
    "\n",
    "# --- 3. è°ƒç”¨ get_data è·å– DataLoader ---\n",
    "print(\"\\n--- è°ƒç”¨ get_data() ---\")\n",
    "data = get_data(args, (dummy_preprocess_fn, None), tokenizer=dummy_tokenizer)\n",
    "dataloader = data['train'].dataloader\n",
    "print(\"âœ… æˆåŠŸåˆ›å»ºDataLoaderï¼\")\n",
    "\n",
    "# --- 4. ä»DataLoaderä¸­å–å‡ºä¸€ä¸ªæ‰¹æ¬¡å¹¶è¿›è¡Œæœ€ç»ˆéªŒè¯ ---\n",
    "print(\"\\n--- æœ€ç»ˆéªŒè¯ï¼šä»DataLoaderä¸­è·å–ä¸€ä¸ªæ‰¹æ¬¡ ---\")\n",
    "try:\n",
    "    batch = next(iter(dataloader))\n",
    "    \n",
    "    print(\"\\nâœ… [æˆåŠŸ] æˆåŠŸä»DataLoaderä¸­è·å–ä¸€ä¸ªæ‰¹æ¬¡çš„æ•°æ®ï¼\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # --- éªŒè¯ 1: æ£€æŸ¥æ‰¹æ¬¡ç±»å‹ ---\n",
    "    print(f\"æ‰¹æ¬¡ç±»å‹: {type(batch)}\")\n",
    "    assert isinstance(batch, PyGBatch), \"æ‰¹æ¬¡ç±»å‹ä¸æ­£ç¡®ï¼åº”ä¸º torch_geometric.data.Batch\"\n",
    "    print(\"âœ… [ç±»å‹æ­£ç¡®] æ‰¹æ¬¡æ˜¯ torch_geometric.data.Batch å¯¹è±¡ã€‚\")\n",
    "    \n",
    "    # --- éªŒè¯ 2: æ£€æŸ¥æ‰¹æ¬¡å†…å®¹ ---\n",
    "    print(\"\\næ‰¹æ¬¡å†…å®¹æ¦‚è§ˆ:\")\n",
    "    print(batch)\n",
    "    \n",
    "    # --- éªŒè¯ 3: æ£€æŸ¥å…³é”®å±æ€§ ---\n",
    "    print(\"\\nå…³é”®å±æ€§æ£€æŸ¥:\")\n",
    "    print(f\"æ‰¹æ¬¡ä¸­çš„å›¾æ•°é‡ (åº”ç­‰äºbatch_size): {batch.num_graphs}\")\n",
    "    assert batch.num_graphs == args.batch_size, \"æ‰¹æ¬¡ä¸­çš„å›¾æ•°é‡ä¸ç­‰äºbatch_size\"\n",
    "    \n",
    "    total_nodes = sum([NUM_NEIGHBORS + 1] * args.batch_size) # ç†è®ºä¸Šçš„èŠ‚ç‚¹æ€»æ•°\n",
    "    print(f\"æ‰¹æ¬¡ä¸­çš„èŠ‚ç‚¹æ€»æ•°: {batch.num_nodes}\")\n",
    "    # æ³¨æ„ï¼šç”±äºæŸäº›é‚»å±…å¯èƒ½ç¼ºå¤±ï¼Œå®é™…èŠ‚ç‚¹æ•°å¯èƒ½ç•¥å°äºç†è®ºå€¼ï¼Œè¿™é‡Œä¸ä½œä¸¥æ ¼æ–­è¨€\n",
    "    \n",
    "    print(f\"x_image å¼ é‡å½¢çŠ¶: {batch.x_image.shape}\")\n",
    "    assert batch.x_image.shape[0] == batch.num_nodes\n",
    "    assert tuple(batch.x_image.shape[1:]) == (3, IMAGE_SIZE[0], IMAGE_SIZE[1])\n",
    "    \n",
    "    print(f\"x_text å¼ é‡å½¢çŠ¶: {batch.x_text.shape}\")\n",
    "    assert batch.x_text.shape[0] == batch.num_nodes\n",
    "    \n",
    "    print(f\"edge_index å¼ é‡å½¢çŠ¶: {batch.edge_index.shape}\")\n",
    "    \n",
    "    print(f\"batch ç´¢å¼•å‘é‡é•¿åº¦ (åº”ç­‰äºèŠ‚ç‚¹æ€»æ•°): {len(batch.batch)}\")\n",
    "    assert len(batch.batch) == batch.num_nodes\n",
    "    \n",
    "    print(\"\\nâœ… [å†…å®¹æ­£ç¡®] æ‰¹æ¬¡å¯¹è±¡çš„æ‰€æœ‰å…³é”®å±æ€§éƒ½ç¬¦åˆé¢„æœŸï¼\")\n",
    "    print(\"-\" * 50)\n",
    "    print(\"\\nğŸ‰ğŸ‰ğŸ‰ æ­å–œï¼æ•°æ®åŠ è½½ç®¡é“éªŒè¯æˆåŠŸï¼æ‚¨å¯ä»¥æ”¾å¿ƒåœ°å°†è¿™äº›ä»£ç é›†æˆåˆ°open-clipé¡¹ç›®ä¸­äº†ã€‚ğŸ‰ğŸ‰ğŸ‰\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ [å¤±è´¥] åœ¨è·å–æˆ–éªŒè¯æ‰¹æ¬¡æ—¶å‘ç”Ÿé”™è¯¯: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fceabe17",
   "metadata": {},
   "source": [
    "# ä¿®å¤é”™è¯¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1340fa44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æ ¸å¿ƒæ•°æ®å¤„ç†ä»£ç å·²åŠ è½½åˆ°Notebookä¸­ (æœ€ç»ˆä¿®å¤ç‰ˆ)ã€‚\n"
     ]
    }
   ],
   "source": [
    "# å•å…ƒæ ¼ 3: å¤åˆ¶å¹¶ç²˜è´´æˆ‘ä»¬æ­£åœ¨æµ‹è¯•çš„æ ¸å¿ƒä»£ç  (æœ€ç»ˆä¿®å¤ç‰ˆ)\n",
    "\n",
    "import logging\n",
    "import scanpy as sc\n",
    "import webdataset as wds\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch_geometric.data import Data as PyGData, Batch as PyGBatch\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# è¿™æ˜¯æ‚¨è®¡åˆ’æ”¾åœ¨ src/open_clip_train/spaglam_data.py çš„ä»£ç \n",
    "# -------------------------------------------------------------\n",
    "class SpaGLaMWDataset(torch.utils.data.IterableDataset):\n",
    "    def __init__(self, tar_urls, anndata_path, image_processor, tokenizer):\n",
    "        super().__init__()\n",
    "        self.tar_urls = tar_urls\n",
    "        self.anndata_path = anndata_path\n",
    "        self.image_processor = image_processor\n",
    "        self.tokenizer = tokenizer\n",
    "        self.adata = None\n",
    "\n",
    "    def _init_worker(self):\n",
    "        if self.adata is None:\n",
    "            self.adata = sc.read_h5ad(self.anndata_path, backed='r')\n",
    "\n",
    "    def _process_sample(self, sample):\n",
    "        self._init_worker()\n",
    "        center_key = sample['__key__']\n",
    "        \n",
    "        try:\n",
    "            # 1. è§£ç ä¸­å¿ƒç‚¹æ•°æ® (ç°åœ¨æ›´åŠ ç¨³å¥)\n",
    "            center_img = sample['png'] # wds.decode(\"pilrgb\") ä¿è¯äº†è¿™æ˜¯ä¸€ä¸ªPIL Image\n",
    "            \n",
    "            # ------------------- æ–‡æœ¬è§£ç æœ€ç»ˆä¿®å¤ç‚¹ -------------------\n",
    "            # é˜²å¾¡æ€§ç¼–ç¨‹ï¼šæ£€æŸ¥ç±»å‹ï¼Œå› ä¸ºwebdatasetå¯èƒ½å·²ç»è‡ªåŠ¨è§£ç äº†.txtæ–‡ä»¶\n",
    "            txt_data = sample['txt']\n",
    "            if isinstance(txt_data, bytes):\n",
    "                center_sent = txt_data.decode('utf-8')\n",
    "            else:\n",
    "                center_sent = txt_data  # å®ƒå·²ç»æ˜¯å­—ç¬¦ä¸²äº†\n",
    "            # ---------------------------------------------------------\n",
    "\n",
    "            # 2. ä» AnnData è·å–é‚»å±…ä¿¡æ¯\n",
    "            center_idx = self.adata.obs_names.get_loc(center_key)\n",
    "            neighbor_indices = self.adata.obsp['spatial_connectivities'][center_idx].indices\n",
    "            neighbor_keys = self.adata.obs_names[neighbor_indices].tolist()\n",
    "            \n",
    "            all_keys = [center_key] + neighbor_keys\n",
    "            num_nodes = len(all_keys)\n",
    "            \n",
    "            paths_df = self.adata.obs.loc[all_keys, ['image_path', 'sentence_path']]\n",
    "            \n",
    "            # 3. åŠ è½½æ‰€æœ‰èŠ‚ç‚¹çš„æ•°æ®å¹¶é¢„å¤„ç†\n",
    "            images_processed, sentences = [], []\n",
    "            for key in all_keys:\n",
    "                if key == center_key:\n",
    "                    img = center_img\n",
    "                    sent = center_sent\n",
    "                else:\n",
    "                    img_path, sent_path = paths_df.loc[key]\n",
    "                    try: img = Image.open(img_path).convert(\"RGB\")\n",
    "                    except (FileNotFoundError, OSError): img = Image.new('RGB', (224, 224), color='grey')\n",
    "                    try: \n",
    "                        with open(sent_path, 'r') as f: sent = f.read()\n",
    "                    except (FileNotFoundError, OSError): sent = \"\"\n",
    "                \n",
    "                images_processed.append(self.image_processor(img))\n",
    "                sentences.append(sent)\n",
    "\n",
    "            tokenized_texts = self.tokenizer(sentences)\n",
    "            \n",
    "            # 4. æ„å»ºå±€éƒ¨å›¾ç»“æ„\n",
    "            key_to_local_idx = {key: i for i, key in enumerate(all_keys)}\n",
    "            edge_list = []\n",
    "            \n",
    "            for i, key in enumerate(all_keys):\n",
    "                global_idx = self.adata.obs_names.get_loc(key)\n",
    "                row_indices = self.adata.obsp['spatial_connectivities'][global_idx].indices\n",
    "                \n",
    "                for neighbor_global_idx in row_indices:\n",
    "                    neighbor_key = self.adata.obs_names[neighbor_global_idx]\n",
    "                    if neighbor_key in key_to_local_idx:\n",
    "                        j = key_to_local_idx[neighbor_key]\n",
    "                        if i < j:\n",
    "                            edge_list.append([i, j])\n",
    "                            edge_list.append([j, i])\n",
    "\n",
    "            edge_index = torch.tensor(edge_list, dtype=torch.long).t().contiguous() if edge_list else torch.empty((2, 0), dtype=torch.long)\n",
    "            \n",
    "            # 5. è¿”å›PyG Dataå¯¹è±¡\n",
    "            return PyGData(\n",
    "                x_image=torch.stack(images_processed),\n",
    "                x_text=tokenized_texts,\n",
    "                edge_index=edge_index,\n",
    "                num_nodes=num_nodes,\n",
    "                center_key=center_key,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing sample {center_key}: {e}\", exc_info=True)\n",
    "            return None\n",
    "\n",
    "    def __iter__(self):\n",
    "        pipeline = wds.DataPipeline(\n",
    "            wds.shardlists.SimpleShardList(self.tar_urls),\n",
    "            wds.split_by_worker,\n",
    "            wds.tarfile_to_samples(),\n",
    "            wds.decode(\"pilrgb\"), # åªè§£ç å›¾åƒï¼Œæ–‡æœ¬ä¿æŒåŸæ ·\n",
    "            wds.map(self._process_sample),\n",
    "            wds.select(lambda x: x is not None),\n",
    "        )\n",
    "        return iter(pipeline)\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# å…¶ä»–ä»£ç  (get_spaglam_dataset, get_dataset_fn, get_data) ä¿æŒä¸å˜\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "def get_spaglam_dataset(args, preprocess_fn, is_train, epoch=0, tokenizer=None):\n",
    "    # ... (æ­¤å‡½æ•°æ— éœ€ä¿®æ”¹)\n",
    "    input_shards = args.train_data\n",
    "    dataset = SpaGLaMWDataset(\n",
    "        tar_urls=input_shards,\n",
    "        anndata_path=args.anndata_path,\n",
    "        image_processor=preprocess_fn,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        num_workers=args.workers,\n",
    "        collate_fn=PyGBatch.from_data_list,\n",
    "        drop_last=is_train,\n",
    "    )\n",
    "    dataloader.num_samples = args.train_num_samples\n",
    "    if dataloader.num_samples:\n",
    "        dataloader.num_batches = dataloader.num_samples // args.batch_size\n",
    "    else:\n",
    "        dataloader.num_batches = 0\n",
    "    return DataInfo(dataloader=dataloader)\n",
    "\n",
    "def get_dataset_fn(data_path, dataset_type):\n",
    "    # ... (æ­¤å‡½æ•°æ— éœ€ä¿®æ”¹)\n",
    "    if dataset_type == \"spaglam\":\n",
    "        return get_spaglam_dataset\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported dataset type for this test: {dataset_type}\")\n",
    "\n",
    "def get_data(args, preprocess_fns, epoch=0, tokenizer=None):\n",
    "    # ... (æ­¤å‡½æ•°æ— éœ€ä¿®æ”¹)\n",
    "    preprocess_train, preprocess_val = preprocess_fns\n",
    "    data = {}\n",
    "    if args.train_data:\n",
    "        data[\"train\"] = get_dataset_fn(args.train_data, args.dataset_type)(\n",
    "            args, preprocess_train, is_train=True, epoch=epoch, tokenizer=tokenizer)\n",
    "    return data\n",
    "\n",
    "print(\"âœ… æ ¸å¿ƒæ•°æ®å¤„ç†ä»£ç å·²åŠ è½½åˆ°Notebookä¸­ (æœ€ç»ˆä¿®å¤ç‰ˆ)ã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f82736f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- æ¨¡æ‹Ÿå‚æ•° ---\n",
      "dataset_type: spaglam\n",
      "train_data: ['/cwStorage/nodecw_group/jijh/openclip_train/spaglam_test_environment/shards/shard-000000.tar']\n",
      "anndata_path: /cwStorage/nodecw_group/jijh/openclip_train/spaglam_test_environment/metadata/dummy_master_adata.h5ad\n",
      "train_num_samples: 100\n",
      "batch_size: 4\n",
      "workers: 0\n",
      "world_size: 1\n",
      "distributed: False\n",
      "\n",
      "âœ… æ¨¡æ‹Ÿçš„é¢„å¤„ç†å™¨å’Œåˆ†è¯å™¨å·²å‡†å¤‡å°±ç»ªã€‚\n",
      "\n",
      "--- è°ƒç”¨ get_data() ---\n",
      "âœ… æˆåŠŸåˆ›å»ºDataLoaderï¼\n",
      "\n",
      "--- æœ€ç»ˆéªŒè¯ï¼šä»DataLoaderä¸­è·å–ä¸€ä¸ªæ‰¹æ¬¡ ---\n",
      "\n",
      "âœ… [æˆåŠŸ] æˆåŠŸä»DataLoaderä¸­è·å–ä¸€ä¸ªæ‰¹æ¬¡çš„æ•°æ®ï¼\n",
      "--------------------------------------------------\n",
      "æ‰¹æ¬¡ç±»å‹: <class 'abc.DataBatch'>\n",
      "âœ… [ç±»å‹æ­£ç¡®] æ‰¹æ¬¡æ˜¯ torch_geometric.data.Batch å¯¹è±¡ã€‚\n",
      "\n",
      "æ‰¹æ¬¡å†…å®¹æ¦‚è§ˆ:\n",
      "DataBatch(edge_index=[2, 106], x_image=[28, 3, 224, 224], x_text=[28, 77], num_nodes=28, center_key=[4], batch=[28], ptr=[5])\n",
      "\n",
      "å…³é”®å±æ€§æ£€æŸ¥:\n",
      "æ‰¹æ¬¡ä¸­çš„å›¾æ•°é‡ (åº”ç­‰äºbatch_size): 4\n",
      "æ‰¹æ¬¡ä¸­çš„èŠ‚ç‚¹æ€»æ•°: 28\n",
      "x_image å¼ é‡å½¢çŠ¶: torch.Size([28, 3, 224, 224])\n",
      "x_text å¼ é‡å½¢çŠ¶: torch.Size([28, 77])\n",
      "edge_index å¼ é‡å½¢çŠ¶: torch.Size([2, 106])\n",
      "batch ç´¢å¼•å‘é‡é•¿åº¦ (åº”ç­‰äºèŠ‚ç‚¹æ€»æ•°): 28\n",
      "\n",
      "âœ… [å†…å®¹æ­£ç¡®] æ‰¹æ¬¡å¯¹è±¡çš„æ‰€æœ‰å…³é”®å±æ€§éƒ½ç¬¦åˆé¢„æœŸï¼\n",
      "--------------------------------------------------\n",
      "\n",
      "ğŸ‰ğŸ‰ğŸ‰ æ­å–œï¼æ•°æ®åŠ è½½ç®¡é“éªŒè¯æˆåŠŸï¼æ‚¨å¯ä»¥æ”¾å¿ƒåœ°å°†è¿™äº›ä»£ç é›†æˆåˆ°open-clipé¡¹ç›®ä¸­äº†ã€‚ğŸ‰ğŸ‰ğŸ‰\n"
     ]
    }
   ],
   "source": [
    "# å•å…ƒæ ¼ 4: æ¨¡æ‹Ÿè®­ç»ƒè„šæœ¬çš„è°ƒç”¨å¹¶éªŒè¯è¾“å‡º\n",
    "\n",
    "# --- 1. æ¨¡æ‹Ÿå‘½ä»¤è¡Œå‚æ•° (args) ---\n",
    "from argparse import Namespace\n",
    "args = Namespace(\n",
    "    dataset_type='spaglam',\n",
    "    train_data=[DUMMY_SHARD_PATH_PATTERN % 0], # ä½¿ç”¨æˆ‘ä»¬åˆ›å»ºçš„æ¨¡æ‹Ÿåˆ†ç‰‡\n",
    "    anndata_path=DUMMY_ADATA_PATH,            # ä½¿ç”¨æˆ‘ä»¬åˆ›å»ºçš„æ¨¡æ‹ŸAnnData\n",
    "    train_num_samples=NUM_SPOTS,\n",
    "    batch_size=4,\n",
    "    workers=0, # å…³é”®ï¼šåœ¨notebookä¸­è®¾ä¸º0ï¼Œä»¥ä¾¿äºè°ƒè¯•ï¼Œé¿å…å¤šè¿›ç¨‹é—®é¢˜\n",
    "    world_size=1,\n",
    "    distributed=False\n",
    ")\n",
    "print(\"--- æ¨¡æ‹Ÿå‚æ•° ---\")\n",
    "for k, v in vars(args).items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "# --- 2. æ¨¡æ‹Ÿé¢„å¤„ç†å™¨å’Œåˆ†è¯å™¨ ---\n",
    "from torchvision import transforms\n",
    "dummy_preprocess_fn = transforms.Compose([\n",
    "    transforms.Resize(IMAGE_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# ä¸€ä¸ªéå¸¸ç®€å•çš„åˆ†è¯å™¨ï¼Œè¿”å›å›ºå®šé•¿åº¦çš„éšæœºæ•´æ•°å¼ é‡\n",
    "dummy_tokenizer = lambda texts: torch.randint(0, 49408, (len(texts), 77), dtype=torch.long)\n",
    "print(\"\\nâœ… æ¨¡æ‹Ÿçš„é¢„å¤„ç†å™¨å’Œåˆ†è¯å™¨å·²å‡†å¤‡å°±ç»ªã€‚\")\n",
    "\n",
    "\n",
    "# --- 3. è°ƒç”¨ get_data è·å– DataLoader ---\n",
    "print(\"\\n--- è°ƒç”¨ get_data() ---\")\n",
    "data = get_data(args, (dummy_preprocess_fn, None), tokenizer=dummy_tokenizer)\n",
    "dataloader = data['train'].dataloader\n",
    "print(\"âœ… æˆåŠŸåˆ›å»ºDataLoaderï¼\")\n",
    "\n",
    "# --- 4. ä»DataLoaderä¸­å–å‡ºä¸€ä¸ªæ‰¹æ¬¡å¹¶è¿›è¡Œæœ€ç»ˆéªŒè¯ ---\n",
    "print(\"\\n--- æœ€ç»ˆéªŒè¯ï¼šä»DataLoaderä¸­è·å–ä¸€ä¸ªæ‰¹æ¬¡ ---\")\n",
    "try:\n",
    "    batch = next(iter(dataloader))\n",
    "    \n",
    "    print(\"\\nâœ… [æˆåŠŸ] æˆåŠŸä»DataLoaderä¸­è·å–ä¸€ä¸ªæ‰¹æ¬¡çš„æ•°æ®ï¼\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # --- éªŒè¯ 1: æ£€æŸ¥æ‰¹æ¬¡ç±»å‹ ---\n",
    "    print(f\"æ‰¹æ¬¡ç±»å‹: {type(batch)}\")\n",
    "    assert isinstance(batch, PyGBatch), \"æ‰¹æ¬¡ç±»å‹ä¸æ­£ç¡®ï¼åº”ä¸º torch_geometric.data.Batch\"\n",
    "    print(\"âœ… [ç±»å‹æ­£ç¡®] æ‰¹æ¬¡æ˜¯ torch_geometric.data.Batch å¯¹è±¡ã€‚\")\n",
    "    \n",
    "    # --- éªŒè¯ 2: æ£€æŸ¥æ‰¹æ¬¡å†…å®¹ ---\n",
    "    print(\"\\næ‰¹æ¬¡å†…å®¹æ¦‚è§ˆ:\")\n",
    "    print(batch)\n",
    "    \n",
    "    # --- éªŒè¯ 3: æ£€æŸ¥å…³é”®å±æ€§ ---\n",
    "    print(\"\\nå…³é”®å±æ€§æ£€æŸ¥:\")\n",
    "    print(f\"æ‰¹æ¬¡ä¸­çš„å›¾æ•°é‡ (åº”ç­‰äºbatch_size): {batch.num_graphs}\")\n",
    "    assert batch.num_graphs == args.batch_size, \"æ‰¹æ¬¡ä¸­çš„å›¾æ•°é‡ä¸ç­‰äºbatch_size\"\n",
    "    \n",
    "    total_nodes = sum([NUM_NEIGHBORS + 1] * args.batch_size) # ç†è®ºä¸Šçš„èŠ‚ç‚¹æ€»æ•°\n",
    "    print(f\"æ‰¹æ¬¡ä¸­çš„èŠ‚ç‚¹æ€»æ•°: {batch.num_nodes}\")\n",
    "    # æ³¨æ„ï¼šç”±äºæŸäº›é‚»å±…å¯èƒ½ç¼ºå¤±ï¼Œå®é™…èŠ‚ç‚¹æ•°å¯èƒ½ç•¥å°äºç†è®ºå€¼ï¼Œè¿™é‡Œä¸ä½œä¸¥æ ¼æ–­è¨€\n",
    "    \n",
    "    print(f\"x_image å¼ é‡å½¢çŠ¶: {batch.x_image.shape}\")\n",
    "    assert batch.x_image.shape[0] == batch.num_nodes\n",
    "    assert tuple(batch.x_image.shape[1:]) == (3, IMAGE_SIZE[0], IMAGE_SIZE[1])\n",
    "    \n",
    "    print(f\"x_text å¼ é‡å½¢çŠ¶: {batch.x_text.shape}\")\n",
    "    assert batch.x_text.shape[0] == batch.num_nodes\n",
    "    \n",
    "    print(f\"edge_index å¼ é‡å½¢çŠ¶: {batch.edge_index.shape}\")\n",
    "    \n",
    "    print(f\"batch ç´¢å¼•å‘é‡é•¿åº¦ (åº”ç­‰äºèŠ‚ç‚¹æ€»æ•°): {len(batch.batch)}\")\n",
    "    assert len(batch.batch) == batch.num_nodes\n",
    "    \n",
    "    print(\"\\nâœ… [å†…å®¹æ­£ç¡®] æ‰¹æ¬¡å¯¹è±¡çš„æ‰€æœ‰å…³é”®å±æ€§éƒ½ç¬¦åˆé¢„æœŸï¼\")\n",
    "    print(\"-\" * 50)\n",
    "    print(\"\\nğŸ‰ğŸ‰ğŸ‰ æ­å–œï¼æ•°æ®åŠ è½½ç®¡é“éªŒè¯æˆåŠŸï¼æ‚¨å¯ä»¥æ”¾å¿ƒåœ°å°†è¿™äº›ä»£ç é›†æˆåˆ°open-clipé¡¹ç›®ä¸­äº†ã€‚ğŸ‰ğŸ‰ğŸ‰\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ [å¤±è´¥] åœ¨è·å–æˆ–éªŒè¯æ‰¹æ¬¡æ—¶å‘ç”Ÿé”™è¯¯: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5457ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (gigapath)",
   "language": "python",
   "name": "gigapath"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
